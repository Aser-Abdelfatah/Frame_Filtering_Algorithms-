{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13722,"status":"ok","timestamp":1682282005442,"user":{"displayName":"Aser Atawya","userId":"12920339476840237433"},"user_tz":420},"id":"iPLyVTNkKow-","outputId":"eb592dff-3a11-4f1d-91a7-330d3144ed5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.4\n"]}],"source":["! pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4051,"status":"ok","timestamp":1682283634022,"user":{"displayName":"Aser Atawya","userId":"12920339476840237433"},"user_tz":420},"id":"4NO7Hs6DeUAL","outputId":"4d8a2edd-2569-45e6-8ce8-39a0d56798d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import torch\n","import tensorflow\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchmetrics\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torchmetrics import StructuralSimilarityIndexMeasure\n","import cv2\n","import time\n","import numpy as np\n","from PIL import Image\n","from google.colab.patches import cv2_imshow\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Y_lzmBfOhYq6"},"source":["# Set up the model\n","- define classes\n","- load model\n","- define predict function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3NkJ5bfhV-P"},"outputs":[],"source":["# Define COCO class names\n","# We won't use this but it's here if you need to sanity check \n","coco_names = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7518,"status":"ok","timestamp":1682282039442,"user":{"displayName":"Aser Atawya","userId":"12920339476840237433"},"user_tz":420},"id":"LefbZ6ypgVIp","outputId":"e4d60a94-e9c2-4d3c-f331-0c99bcf0f075"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth\" to /root/.cache/torch/hub/checkpoints/ssd300_vgg16_coco-b556d3b4.pth\n","100%|██████████| 136M/136M [00:01<00:00, 97.5MB/s]\n"]}],"source":["# Define the torchvision image transforms\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","# Load the object detection model, SSD, and set mode to eval\n","model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n","model.eval()\n","model = model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWpVbo6Jgbyx"},"outputs":[],"source":["# Function to run a single image through model and get boxes, labels, and scores\n","def predict(image, model, detection_threshold):\n","    # transform the image to tensor\n","    image = transform(image)\n","\n","    # add a batch dimension\n","    image = image.unsqueeze(0) \n","\n","    image = image.to(\"cuda\")\n","    # get the predictions on the image\n","    outputs = model(image) \n","\n","    # get score for all the predicted objects\n","    pred_scores = outputs[0]['scores'].to(\"cpu\")\n","    pred_scores = pred_scores.detach()\n","\n","    # get all the predicted bounding boxes and filter by threshold\n","    pred_bboxes = outputs[0]['boxes'].to(\"cpu\")\n","    pred_bboxes = pred_bboxes.detach()\n","    boxes = pred_bboxes[pred_scores >= detection_threshold]\n","\n","    # get all predicted labels and filter by threshold    \n","    labels = outputs[0]['labels'].to(\"cpu\")\n","    labels = labels[pred_scores >= detection_threshold]\n","\n","    scores = pred_scores[pred_scores >= detection_threshold]\n","\n","    return boxes, labels, scores "]},{"cell_type":"markdown","metadata":{"id":"Xp0Ou2Tmkamr"},"source":["# TESTING\n","\n","If you need to sanity check your predictions, you can use this section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPMC0_GmkZzy"},"outputs":[],"source":["image = Image.open('/content/drive/My Drive/CS181_FINAL_PROJECT/Resources/.jpeg')\n","boxes, labels, scores = predict(image, model, 0.3)\n","print(boxes)\n","print(scores)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYQeHMHScvB9"},"outputs":[],"source":["COLORS = np.random.uniform(0, 255, size=(91, 3))\n","def draw_boxes(boxes, labels, image):\n","    \"\"\"\n","    Draws the bounding box around a detected object.\n","    \"\"\"\n","    image = cv2.cvtColor(np.asarray(image), cv2.COLOR_BGR2RGB)\n","    for i, box in enumerate(boxes):\n","        color = COLORS[labels[i]]\n","        cv2.rectangle(\n","            image,\n","            (int(box[0]), int(box[1])),\n","            (int(box[2]), int(box[3])),\n","            color, 2\n","        )\n","        cv2.putText(image, str(labels[i]), (int(box[0]), int(box[1]-5)),\n","                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n","                    lineType=cv2.LINE_AA)\n","    return image\n","\n","image = draw_boxes(boxes, labels, image)\n","cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"jQbtW05ojiiR"},"source":["# Accuracy Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZFE8P9YjmYj"},"outputs":[],"source":["# Function that takes \"ground truth\" boxes and labels and \"comparison\" boxes, labels,\n","# and scores, and returns the accuracy of the comparison result relative to the ground truth "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTIshif0rXFf"},"outputs":[],"source":["def calculate_accuracy(ground_truth, prediction):\n","    metric = MeanAveragePrecision(iou_type=\"bbox\")\n","    metric.update(prediction, ground_truth)\n","    result = metric.compute()\n","    return result['map'].item()"]},{"cell_type":"markdown","metadata":{"id":"4qGGTNtyedqx"},"source":["# Read video and process frames\n","- Read in a video and loop through its frames using the OpenCV library\n","- Run the predict function on each frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DclS-6Vqgh2g"},"outputs":[],"source":["\n","def should_process_frame(frame, prev_frame, index, policy, threshold, blurred):\n","\n","  if (index == 0) : return True\n","  \n","  # prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","  # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","  \n","  if(blurred):\n","    prev_frame = cv2.GaussianBlur(prev_frame, (3, 3), 0)\n","    frame = cv2.GaussianBlur(frame, (3, 3), 0)\n","  \n","  # Applying the function mse\n","  if(policy == \"mse\"):\n","    def mse(img1, img2):\n","      h, w, d = img1.shape\n","      diff = cv2.subtract(img1, img2)\n","      # cv2_imshow(diff)\n","      err = np.sum(diff**2)\n","      mse = err/(float(h*w*d))\n","      return mse, diff\n","    error, diff = mse(prev_frame, frame)\n","\n","  # Applying the function mae\n","  if(policy == \"mae\"):\n","    def mae(img1, img2):\n","      h, w, d = img1.shape\n","      diff = cv2.subtract(img1, img2)\n","      err = np.sum(abs(diff))\n","      mae = err/(float(h*w*d))\n","      return mae, diff\n","    error, diff = mae(prev_frame, frame)\n","\n","  # Applying the function ssim\n","  if(policy == \"ssim\"):\n","    def ssim(img1, img2):\n","      func = StructuralSimilarityIndexMeasure()\n","      img1 = torch.from_numpy(img1)\n","      # add the batch dimesnion\n","      img1 = torch.unsqueeze(img1, dim = 0)\n","      # reorder tensor to follow the dimesnions B*C*H*W; it's orginially B*H*W*C\n","      img1 = torch.movedim(img1, (0,1,2,3), (0,2,3, 1))\n","      img1 = img1.float()\n","      img2 = torch.from_numpy(img2)\n","      # add the batch dimesnion\n","      img2 = torch.unsqueeze(img2, dim = 0)\n","      # reorder tensor to follow the dimesnions B*C*H*W; it's orginially B*H*W*C\n","      img2 = torch.movedim(img2, (0,1,2,3),(0,2,3, 1))\n","      img2 = img2.float()\n","      similarity = func(img1, img2)\n","      return similarity.item()\n","    error = ssim(prev_frame, frame)\n","\n"," \n","  \n","  # print(error)\n","  if (error < threshold): return False \n","  else: return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6YF8hkJfy2K"},"outputs":[],"source":["def process_video(video_path, model):\n","        # Ask user for filtering policy and ensure it's valid\n","        policy = input('Enter the filtering policy (mse, mae, ssim): ')\n","        if policy not in ['mse', 'mae', 'ssim']:\n","          print('Please enter a valid filtering policy')\n","          return None, None\n","\n","        blurred = False\n","        blurredInput = input('Would you like the frame Blurred? (Y/N)')\n","        if blurredInput == \"Y\" or blurredInput == \"Yes\" or blurredInput == \"y\" or blurredInput == \"yes\":\n","          blurred = True  \n","\n","        threshold = int(input('Enter the THRESHOLD: '))\n","\n","        ground_truth = {}\n","        comparison = {}\n","\n","        # Set up video capture\n","        cap = cv2.VideoCapture(video_path)\n","        frame_count = int(cv2.VideoCapture.get(cap, int(cv2.CAP_PROP_FRAME_COUNT)))\n","        print(f'Frame count: {frame_count}')\n","        ret, frame = cap.read()\n","        prev_frame = None\n","\n","        index = -1\n","        start_time = time.time()\n","\n","        frames_filtered  = 0\n","\n","        # Store the results of the last processed frame. If we skip a frame, fill \"comparison\" with the \n","        # last processed frame\n","        prev_frame_results = (None, None, None)\n","\n","        # Loop through frames\n","        while cap.isOpened():\n","            if not ret:\n","                break\n","            index += 1\n","             \n","            if index%100 == 0:\n","              print(f'Reached index {index}')\n","              # if(index != 0) : print(f'PercentageFilteredSoFar: {frames_filtered/index}')\n","\n","            image_id = f'image{index}'\n","            ground_truth[image_id] = {}\n","            comparison[image_id] = {}\n","\n","            # Run prediction on this frame. We have to run it regardless of our filtering\n","            # method so we can assess ground truth\n","            boxes, labels, scores = predict(frame, model, 0.3)\n","            \n","            ground_truth[image_id]['boxes'] = boxes\n","            ground_truth[image_id]['labels'] = labels\n","\n","            if should_process_frame(frame, prev_frame, index, policy, threshold, blurred):\n","                # This becomes the last frame processed\n","                prev_frame_results = (boxes, labels, scores)\n","                prev_frame = frame.copy()\n","\n","                # Comparison contains results of this frame\n","                comparison[image_id]['boxes'] = boxes\n","                comparison[image_id]['labels'] = labels\n","                comparison[image_id]['scores'] = scores\n","            else:\n","                frames_filtered += 1\n","                # Use the previous frame's results instead\n","                comparison[image_id]['boxes'] = prev_frame_results[0]\n","                comparison[image_id]['labels'] = prev_frame_results[1]\n","                comparison[image_id]['scores'] = prev_frame_results[2]\n","            ret, frame = cap.read()\n","\n","        end_time = time.time()\n","        print(f'Total time: {end_time - start_time}')\n","        print(f'Threshold: {threshold}')\n","        print(f'Frame Percentage Filtered Out: { frames_filtered / frame_count}')\n","        \n","        # Calculate percentage of frames selected and write indexes of those to a json file\n","        cap.release()\n","\n","        ground_truth_formatted = [v for k,v in ground_truth.items()]\n","        comparison_formatted = [v for k,v in comparison.items()]\n","\n","        # print(f'Ground truth: {ground_truth_formatted}')\n","        # print(f'Comparison: {comparison_formatted}')\n","\n","        return ground_truth_formatted, comparison_formatted"]},{"cell_type":"markdown","metadata":{"id":"a9AJtnQzKy4t"},"source":["# Run the video through the model and get the result!\n","\n","This is the code block you'll want to rerun as you change configurations and videos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"elapsed":76592,"status":"error","timestamp":1682285855507,"user":{"displayName":"Aser Atawya","userId":"12920339476840237433"},"user_tz":420},"id":"wXaYd8ezl49H","outputId":"76dd37a4-1fe8-46ff-b1e5-b4484ac89e35"},"outputs":[],"source":["# In order to work through sharing. Go to the folder CS181_FINAL_PROJECT and select (Add Shortcut To Drive) and add it to your Colab Notebooks folder. \n","# then the below code should work without error. \n","gt, comp = process_video('/content/drive/My Drive/Colab Notebooks/CS181_FINAL_PROJECT/Resources/old/Banff.mp4', model)\n","mAP = calculate_accuracy(gt, comp)\n","\n","print(f'mAP: { mAP}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Xp0Ou2Tmkamr"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
